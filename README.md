# ğŸ“Š MoMA Art Collection Analytics - Data Engineering Project

> An end-to-end data pipeline that transforms MoMA's public art collection into an analytics-ready data warehouse, enriched with historical, economic, and biographical context.

## ğŸ¯ Project Overview

This project demonstrates **production-grade data engineering practices** using real-world museum data:

- **Orchestrated with Apache Airflow** - 5 modular DAGs managing dependencies and parallel execution
- **Multi-layered architecture** - Staging â†’ Transformation â†’ Dimensional (star schema for BI)
- **Multi-source enrichment** - Combined MoMA data with 3 external APIs (Wikidata, FRED, REST Countries)

**What it does:**
Processes ~140,000 artworks and ~15,000 artists, adding geographic regions, economic context (GDP, recessions), and artist biographies to enable to enable complex analytical reporting.

## ğŸ—ï¸ Architecture Highlights

### Three-Layer Data Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     STAGING LAYER                           â”‚
â”‚  Raw data landing zone - exact copy from sources            â”‚
â”‚  â€¢ MoMA CSVs (Artworks, Artists)                            â”‚
â”‚  â€¢ Wikidata API responses (artist biographies)              â”‚
â”‚  â€¢ Economic data (FRED API)                                 â”‚
â”‚  â€¢ Geographic reference (REST Countries API)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  TRANSFORMATION LAYER                       â”‚
â”‚  Cleaned, standardized, quality-scored data                 â”‚
â”‚  â€¢ Data type conversions (text â†’ integers, dates)           â”‚
â”‚  â€¢ Standardization (gender, nationality)                    â”‚
â”‚  â€¢ Enrichment (geo mapping, Wikidata merging)               â”‚
â”‚  â€¢ Quality scoring (completeness metrics)                   â”‚
â”‚  â€¢ Bridge tables (many-to-many relationships)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   DIMENSIONAL LAYER                         â”‚
â”‚  Star schema optimized for analytics (BI-ready)             â”‚
â”‚  â€¢ Dimension Tables: Artist, Artwork, Date, Geography       â”‚
â”‚  â€¢ Fact Tables: Acquisitions, Artist Summary                â”‚
â”‚  â€¢ Pre-aggregated metrics for fast queries                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
---

## ğŸ—‚ï¸ Database Schema - Entity Relationship Diagrams

### 1. Staging Layer - Raw Data Landing Zone

The staging layer stores unprocessed data exactly as received from sources. All columns are stored as TEXT to preserve original formats.

![Staging Schema ERD](MoMa_dwh_Staging_ERD.png)

**Key Tables:**
- `staging_moma_artworks` - Raw artwork data from GitHub CSV (~140K rows)
- `staging_moma_artists` - Raw artist data from GitHub CSV (~15K rows)
- `staging_wikidata_artists` - Enrichment data from Wikidata API (batch SPARQL queries)
- `staging_geographic_reference` - Country/continent mappings from REST Countries API
- `staging_economic_data` - Historical GDP, inflation, unemployment from FRED API

---

### 2. Transformation Layer - Cleaned & Enriched Data

The transformation layer applies data cleaning, type conversions, standardization, and enrichment. Includes quality scoring and bridge tables for many-to-many relationships.

![Transformed Schema ERD](MoMa_dwh_Transformed_ERD.png)

**Key Tables:**
- `transformed_artworks` - Cleaned artwork data with parsed dates, dimensions, and quality scores
- `transformed_artists` - Enriched artist data with geographic context, movements, and Wikidata bio
- `bridge_artwork_artist` - Many-to-many relationship (handles collaborative artworks)
- `transformed_geography` - Geographic reference with artist/artwork counts by nationality
- `transformed_date_economics` - Date dimension with economic context (GDP, recessions, eras)

**Key Transformations:**
- Date parsing: "c. 1920" â†’ year=1920, certainty='circa'
- Gender standardization: "M", "(Male)" â†’ "Male"
- Nationality mapping: "American" â†’ country="United States", continent="North America"
- Quality scoring: Automated completeness metrics (0.0-1.0)

---

### 3. Dimensional Layer - Star Schema for Analytics

The dimensional layer implements a star schema optimized for BI queries. Fact tables reference dimension tables through foreign keys.

![Dimensional Schema ERD](MoMa_dwh_Dimensional_ERD.png)

**Dimension Tables:**
- `dim_artist` - Artist dimension (SCD Type 0 - no history tracking)
- `dim_artwork` - Artwork dimension with physical attributes
- `dim_date` - Date dimension with economic and historical context
- `dim_geography` - Geographic dimension keyed by nationality

**Fact Tables:**
- `fact_artwork_acquisitions` - One row per artwork acquisition with context
- `fact_artist_summary` - Aggregate fact table (one row per artist)

**Star Schema Benefits:**
- Fast joins (denormalized dimensions)
- Simple queries (business users can write SQL)
- Pre-calculated metrics (years_from_creation_to_acquisition)
- Optimized for aggregations

---

## ğŸ“Š Data Flow

### Complete Pipeline Journey

```
1. DATA ACQUISITION
   â”œâ”€ Download MoMA CSVs from GitHub
   â”œâ”€ Fetch Wikidata biographies
   â”œâ”€ Fetch economic indicators (FRED API)
   â””â”€ Fetch country mappings (REST Countries API)
                    â†“
2. STAGING LOAD
   â”œâ”€ Load raw CSVs to PostgreSQL (no transformations)
   â”œâ”€ Store API responses with metadata
   â””â”€ Preserve raw source data
                    â†“
3. TRANSFORMATION
   â”œâ”€ Artists: Clean birth/death years, standardize gender/nationality
   â”œâ”€ Artworks: Parse dates (circa/exact/range), extract dimensions
   â”œâ”€ Geography: Match nationalities to countries/continents
   â”œâ”€ Economics: Classify eras (WWI, Great Depression, etc.)
   â””â”€ Bridge Table: Link artworks â†” artists (many-to-many relationship)
                    â†“
4. DIMENSIONAL MODELING
   â”œâ”€ Build dimension tables (SCD Type 0 - mostly static data)
   â”œâ”€ Create fact tables with foreign keys
   â”œâ”€ Calculate derived metrics (years to acquisition, etc.)
   â””â”€ Add economic context to acquisition facts
                    â†“
5. ANALYTICS-READY DATA
   â””â”€ Can be consumed by BI tools (Tableau, Power BI, etc.)
```

### Key Data Engineering Concepts Demonstrated

#### 1. **Batch SPARQL Queries for API Efficiency**
Instead of fetching Wikidata one artist at a time (15,000 requests), the pipeline batches 50 artists per query:
- **Before**: 15,000 API calls Ã— 1 second = 4+ hours
- **After**: 300 batches Ã— 1 second = 5 minutes

#### 2. **Vectorized Data Transformation**
Uses pandas vectorization instead of row-by-row iteration:
```python
# âŒ Slow: 140,000 iterations
for row in df.iterrows():
    if 'c.' in row['date']:
        row['certainty'] = 'circa'

# âœ… Fast: Single vectorized operation
df.loc[df['date'].str.contains('c.'), 'certainty'] = 'circa'
```
**Result**: Artwork transformation runs in seconds instead of minutes

#### 3. **Bridge Table Pattern**
Handles many-to-many relationships (collaborative artworks):
```
Artwork "Guernica" â”€â”€â”€â”
                      â”œâ”€â†’ Bridge Table â”€â”€â†’ Artist "Picasso"
Artwork "The Kiss" â”€â”€â”€â”˜                  (with role metadata)
```
**Why**: Some artworks can have multiple artists; some artists collaborate frequently

---

## ğŸ”„ Airflow DAG Structure

### Master Orchestration Pipeline

The project uses **5 DAGs** organized in a hierarchical structure for modularity and parallel execution:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          moma_master_pipeline (Orchestrator)                  â”‚
â”‚  Triggers all child DAGs in sequence, waits for completion    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â†“             â†“             â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Download â”‚  â”‚   Create   â”‚  â”‚     ...      â”‚
        â”‚   Data   â”‚  â”‚  Schemas   â”‚  â”‚              â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### DAG Hierarchy (Execution Order)

```
1. moma_master_pipeline
   â”œâ”€ Phase 1: Setup (parallel)
   â”‚   â”œâ”€ trigger_download_data         â†’ Downloads CSVs
   â”‚   â””â”€ trigger_create_schemas        â†’ Creates SQL schemas
   â”‚
   â”œâ”€ Phase 2: Staging
   â”‚   â””â”€ trigger_load_staging          â†’ Loads raw csv data + API enrichments to database
   â”‚       â”œâ”€ load_artworks_csv
   â”‚       â”œâ”€ load_artists_csv
   â”‚       â”œâ”€ load_wikidata (parallel after CSVs loaded)
   â”‚       â”œâ”€ load_geographic (parallel)
   â”‚       â””â”€ load_economic (parallel)
   â”‚
   â”œâ”€ Phase 3: Transformation
   â”‚   â””â”€ trigger_load_transformed
   â”‚       â”œâ”€ verify_staging_data (first)
   â”‚       â”œâ”€ transform_artworks â”€â”€â”
   â”‚       â”œâ”€ transform_artists â”€â”€â”€â”¼â”€â†’ (parallel)
   â”‚       â”‚                       â”‚
   â”‚       â””â”€ sync_point â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â”‚           â”œâ”€ build_bridge_table
   â”‚           â”œâ”€ transform_geo (depends from artist table)
   â”‚           â”œâ”€ transform_economic
   â”‚           â””â”€ verify_transformations
   â”‚
   â””â”€ Phase 4: Dimensional Load
       â””â”€ trigger_load_dimensional
           â”œâ”€ Dimensions (parallel)
           â”‚   â”œâ”€ load_dim_date
           â”‚   â”œâ”€ load_dim_geography
           â”‚   â”œâ”€ load_dim_artist
           â”‚   â””â”€ load_dim_artwork
           â”‚
           â”œâ”€ Facts (after dimensions)
           â”‚   â”œâ”€ load_fact_acquisitions
           â”‚   â””â”€ load_fact_artist_summary
           â”‚
           â””â”€ verify_dimensional
```

### Key DAG Design Patterns

#### 1. **Wait-for-Completion Pattern**
```python
trigger_load_staging = TriggerDagRunOperator(
    trigger_dag_id='moma_load_staging_complete',
    wait_for_completion=True,  # Blocks until child DAG finishes
    poke_interval=30           # Checks status every 30 seconds
)
```
**Benefit**: Master DAG orchestrates dependencies; child DAGs stay modular

#### 2. **Parallel Execution with Sync Points**
```python
# Phase 3: Artworks and Artists transform in parallel
[transform_artworks, transform_artists] >> sync_point

# Then dependent tasks run
sync_point >> [build_bridge, transform_geo, transform_economic]
```
**Benefit**: Reduces total runtime (artworks + artists = 10 min parallel vs 20 min sequential)

#### 3. **Verification Tasks**
Each phase has verification:
- `verify_staging_data`: Checks row counts before transformation
- `verify_transformations`: Validates data quality scores
- `verify_dimensional`: Confirms fact/dimension integrity

**Benefit**: Fail fast if data quality issues detected

---

## ğŸ› ï¸ Technology Stack

| Component | Technology | Purpose |
|-----------|-----------|---------|
| **Orchestration** | Apache Airflow | DAG scheduling, dependency management |
| **Database** | PostgreSQL | Staging, transformation, dimensional layers |
| **Data Processing** | Python + Pandas | ETL logic, data transformations |
| **APIs** | Wikidata, FRED, REST Countries | External data enrichment |
| **Schema Design** | Star Schema | Dimensional modeling for BI |

---

## ğŸš€ Quick Start

### Prerequisites
- Python 3.8+
- PostgreSQL 12+
- Apache Airflow 2.0+
- API Keys: [FRED API Key](https://fred.stlouisfed.org/docs/api/api_key.html)

### Setup

1. **Clone repository and install dependencies**
   ```bash
   git clone <repo-url>
   cd moma_project
   pip install -r dependencies.txt
   ```

2. **Configure environment variables** (`.env` file)
   ```env
   DB_HOST=localhost
   DB_PORT=5432
   DB_NAME=moma_analytics
   DB_USER=postgres
   DB_PASSWORD=your_password
   FRED_API_KEY=your_fred_key
   ```

3. **Run the complete pipeline**
   ```bash
   # Option 1: Airflow (orchestrated)
   airflow dags trigger moma_master_pipeline

   # Option 2: Standalone scripts (manual)
   python scripts/download_data.py
   python scripts/execute_sql.py --schema all
   python scripts/load_staging_data.py
   python scripts/transform_artists.py
   python scripts/transform_artworks.py
   python scripts/facts_dimensions.py
   ```

---

## ğŸ“ˆ Sample Analytics Queries

Once the pipeline completes, we can run queries like:

```sql
-- Top 10 most prolific artists
SELECT 
    da.display_name,
    dg.country_name,
    COUNT(*) as artworks
FROM dimensional.fact_artwork_acquisitions faa
JOIN dimensional.dim_artist da ON faa.artist_id = da.artist_id
JOIN dimensional.dim_geography dg ON da.nationality = dg.nationality
GROUP BY da.display_name, dg.country_name
ORDER BY artworks DESC
LIMIT 10;

-- Acquisitions during economic recessions
SELECT 
    dd.year,
    dd.economic_period,
    COUNT(*) as acquisitions
FROM dimensional.fact_artwork_acquisitions faa
JOIN dimensional.dim_date dd ON faa.acquisition_date_key = dd.date_key
WHERE dd.economic_period = 'Recession'
GROUP BY dd.year, dd.economic_period
ORDER BY dd.year;
```

---

## ğŸ“‚ Project Structure

```
moma_project/
â”œâ”€â”€ dags/                          # Airflow DAG definitions
â”‚   â”œâ”€â”€ moma_master_pipeline_dag.py         # Orchestrator
â”‚   â”œâ”€â”€ moma_download_dag.py                # Phase 1 (Setup, Download raw CSVs)
â”‚   â”œâ”€â”€ moma_create_schemas_dag.py          # Phase 1 (Setup, Create schemas)
â”‚   â”œâ”€â”€ moma_load_staging_dag.py            # Phase 2
â”‚   â”œâ”€â”€ moma_load_transformed_dag.py        # Phase 3
â”‚   â””â”€â”€ moma_load_dimensional_dag.py        # Phase 4
â”‚
â”œâ”€â”€ scripts/                       # Python ETL scripts
â”‚   â”œâ”€â”€ download_data.py                    # Download CSV from MoMa Git repository
â”‚   â”œâ”€â”€ load_staging_data.py                # CSV â†’ staging (artworks & artists)
â”‚   â”œâ”€â”€ load_wikidata_artists.py            # Load artists enrichment data (from Wikidata API)
â”‚   â”œâ”€â”€ load_geographic_data.py             # Load geographic enrichment data (from REST Countries API)
â”‚   â”œâ”€â”€ load_economic_data.py               # Load economic enrichment data(from FRED API)
â”‚   â”œâ”€â”€ transform_artists.py                # Artist transformation
â”‚   â”œâ”€â”€ transform_artworks.py               # Artwork transformation
â”‚   â”œâ”€â”€ transform_geo_economic.py           # Geo & economic data transformation
â”‚   â”œâ”€â”€ build_bridge_add_artist_stats.py    # Building bridge table (for artist&artwork relation)
â”‚   â””â”€â”€ facts_dimensions.py                 # Star schema load
â”‚
â”œâ”€â”€ sql/                           # Schema DDL scripts
â”‚   â”œâ”€â”€ staging_ddl.sql
â”‚   â”œâ”€â”€ transformed_ddl.sql
â”‚   â””â”€â”€ dimensional_ddl.sql
â”‚
â”œâ”€â”€ config.py                      # Centralized configuration
â”œâ”€â”€ .env                           # Secrets (not in repo)
â””â”€â”€ README.md                      # This file
```

---

## ğŸ“ Key Learning Outcomes

This project demonstrates:

1. **Multi-layer data architecture** (staging â†’ transformation â†’ dimensional)
2. **Data quality management** (automated verification)
3. **Many-to-many relationships** (bridge table pattern in Transformed schema)
4. **Dimensional modeling** (star schema for analytics)
5. **External data enrichment** (3+ API integrations)
6. **Vectorized transformations** (pandas performance optimization)
7. **Airflow orchestration** with complex dependencies and parallel execution

---
<!-- To create  files:
    - MoMA_Project_-_Dimensional_Layer_Setup_Guide.pdf
    - MoMA_Project_-_Dimensional_Layer_Setup_Guide.pdf
    - MoMA_Project_-_Dimensional_Layer_Setup_Guide.pdf
    - MoMA_Art_Collection_Analytics_-_Data_Engineering_Project_Plan.pdf -->

<!-- ## ğŸ“š Documentation

For detailed technical documentation:
- **[Staging Schema Setup](MoMA_Project_-_Staging_Schema_Setup_Guide.pdf)**: Initial data loading
- **[Transformation Layer](MoMA_Project_-_Transformation_Layer_Setup_Guide.pdf)**: Cleaning and enrichment logic
- **[Dimensional Layer](MoMA_Project_-_Dimensional_Layer_Setup_Guide.pdf)**: Facts and Dimensions tables ready to use for analytial queries
- **[Project Plan](MoMA_Art_Collection_Analytics_-_Data_Engineering_Project_Plan.pdf)**: Original design document -->